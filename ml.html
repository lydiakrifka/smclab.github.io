<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Machine Learning for Sound and Music — SMC Lab</title>
    <style>
        body {
            background-color: #0d0d0d;
            color: #e0e0e0;
            font-family: monospace;
            margin: 2rem;
        }

        a {
            color: #9cf;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        h1,
        h2,
        h3 {
            color: #fff;
        }

        hr {
            border: 0;
            border-top: 1px solid #333;
            margin: 1rem 0;
        }

        .nav {
            margin-bottom: 2rem;
        }

        .nav a {
            margin-right: 1rem;
        }

        pre {
            background: #111;
            padding: 1rem;
            border-radius: 6px;
            overflow-x: auto;
        }
    </style>
</head>

<body>
    <div class="nav">
        <a href="index.html">← Back to Home</a>
    </div>

    <h1>Machine Learning for Sound and Music</h1>
    <p>
        This page collects notes, notebooks, and experiments related to the
        <em>Machine Learning for Sound and Music</em> course at the Sound and
        Music Computation master’s programme, Universitat Pompeu Fabra (UPF),
        Barcelona.
    </p>

    <h2>Overview</h2>
    <p>
        The course introduces key machine learning concepts applied to sound and
        music. It covers supervised, self-supervised, and multimodal learning with
        examples in deep audio processing and creative AI.
    </p>

    <h2>Lecture Index</h2>
    <ul>
        <li>Lecture 5 — Supervised Learning with CNNs</li>
        <li>Lecture 6 — Self-Supervised Learning I: Autoencoders</li>
        <li>
            Lecture 7 — Self-Supervised Representation Learning II: Metric Learning
        </li>
        <li>Lecture 8 — Transformers and the Attention Mechanism</li>
        <li>Lecture 9 — Transfer Learning</li>
        <li>Lecture 10 — Multi-modal Learning: Text-Audio Models</li>
    </ul>

    <h2>Course Structure</h2>
    <ul>
        <li><strong>notebooks/</strong> — Lecture notebooks and examples</li>
        <li><strong>docker/</strong> — Environment for reproducibility</li>
    </ul>

    <h2>References</h2>
    <ul>
        <li>
            <a href="https://github.com/MTG/machine_learning_for_sound_and_music_25">Original MTG repository</a>
        </li>
        <li>
            <a href="https://essentia.upf.edu">Essentia library</a> — Audio feature
            extraction and analysis
        </li>
        <li>
            <a href="https://pytorch.org">PyTorch</a> — Deep learning framework
        </li>
    </ul>

    <hr />
    <p>© 2025 Lydia Krifka-Dobes — SMC Lab</p>
</body>

</html>
